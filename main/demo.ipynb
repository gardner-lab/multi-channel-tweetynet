{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This notebooks illustrates usage of the Tweetynet convolutional neural network for birdsong syllable classification and supporting software. The network processes magnitude spectral windows of raw audio recordings by extracting visual features, downsampling the input via 2D convolution and pooling operations, and finally providing downsampled activation maps as input to an LSTM, which feeds a linear readout unit that classifies discrete time steps as belonging either one of the pre-specified syllable types or a period of non-singing. In this notebook, we illustrate how to generate magnitude spectrograms for a variety of parameterizations, generate labelvectors from song annotation files, create datasets for training and evalution, and build + train a model.\n",
    "\n",
    "This software's unique contribution is in its support of \"wideband\" spectral input to the Tweetynet neural network. More concretely, our implementation of Tweetynet can process a 3-dimensional input spectrogram window where depth is created by stacking copies of the same window that are computed with distinct FFT sizes (and resulting aspect ratios). Tweetynet applies a separate layer 1 convolution and pooling operation to each channel to appropriately featurize and downsample input to matching dimensions prior to the second convolutional layer. In addition, provided tooling for spectrogram and labelvector generation are easily parameterized with wideband input in mind. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "If you have not already done so, proceed to https://figshare.com/articles/dataset/Bengalese_Finch_song_repository/4805749 to collect the dataset most readily compatible with this software package and tutorial.\n",
    "\n",
    "You are free to download the entirety of the dataset; however, all that is required is ```sober.repo1.gy6or6.032212.tar.gz```\n",
    "\n",
    "Once downloaded, unpack the zip file and place the directory ```032212``` in a preferred location in preparation for the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "Before starting visit and review ```/parameters/params.py```. Said file specifies key parameters of this script and its supporting software. At minimum, you will need to hardcode the following named directory parameters:\n",
    "- ```audio_dir_path``` (this should be the path to directory ```032212```)\n",
    "- ```annot_dir_path``` (this should be the path to directory ```032212```)\n",
    "- ```spect_dir_path``` (wherever you want to write spectrograms on disk)\n",
    "\n",
    "That being said, we highly recommend that while working your way through this notebook you frequently reference ```/parameters/params.py``` along with other ```_params.py``` files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Directories\n",
    "Create necessary directories if need be or remove old files from existing directories. Ensure you have already assigned the above listed named directory parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parameters.params import (\n",
    "    WINDOWED_LABELVECS_DIR_PATH,\n",
    "    UNCUT_LABELVECS_DIR_PATH,\n",
    "    WINDOWED_SPECTS_DIR_PATH,\n",
    "    UNCUT_SPECTS_DIR_PATH,\n",
    ")\n",
    "dirs = [\n",
    "    WINDOWED_LABELVECS_DIR_PATH,\n",
    "    UNCUT_LABELVECS_DIR_PATH,\n",
    "    WINDOWED_SPECTS_DIR_PATH,\n",
    "    UNCUT_SPECTS_DIR_PATH,\n",
    "]\n",
    "from src.utilities import setup_directories\n",
    "setup_directories(dirs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Spectrograms\n",
    "For each provided audio file we compute and save to disk a full duration spectrogram as well as consecutive overlapping windows. Short windows are useful for training. Full duration spectrograms can be used for reference and/or model evalution on song-by-song basis. As discussed, this version of Tweetynet supports processing multiple input spectrograms as \"channels\". Accordingly, with a single audio file it is easy to generate multiple spectrograms using different parameterizations to render different output dimensions. Our spectrogram generation procedure includes small quirks to produce output images with an identical number of pixels for different FFT setups. For example, with spectrogram A, B, and C, produced with FFT sizes 256, 512, and 1024, the height (frequency dimension) of C will be twice that of B and four times that of A. An identical relationship holds for the time dimension.\n",
    "\n",
    "NOTE: In addition to actual spectrograms and spectrogram windows this method writes other metadata to disk with each record. Details follow.\n",
    "\n",
    "NOTE: This should take a bit of time depending on the dataset size, number of N_FFTs, and extraction sliding window overlap. With default parameters on a stock Macbook Pro each minute of audio requires 1-1.5 minutes for spectrogram generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.spect_writer import SpectWriter\n",
    "from parameters.spect_writer_params import SPECT_WRITER_PARAMS\n",
    "spect_writer = SpectWriter(**SPECT_WRITER_PARAMS)\n",
    "spect_writer.write()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine Actual Spectrogram Window Sizes \n",
    "In order to instantiate our network, we need to provide input spectrogram shapes. Here, use a utility method to load a sample set of spectrogram windows, extract the shapes, and save them to the 'network_params' dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from src.utilities import get_spect_window_shapes\n",
    "from parameters.params import WINDOWED_SPECTS_DIR_PATH, SPECT_FILE_FMT\n",
    "from parameters.net_params import NETWORK_PARAMS\n",
    "n_ffts = NETWORK_PARAMS[\"n_ffts\"]\n",
    "network_input_shapes = get_spect_window_shapes(WINDOWED_SPECTS_DIR_PATH, SPECT_FILE_FMT, n_ffts)\n",
    "NETWORK_PARAMS[\"input_shapes\"] = network_input_shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Network. Sample 'forward()' Call To Compute Labelvec Length\n",
    "To create label vectors from audio file anntations we need to know the network's output sizes for predetermined input sizes. Instiate the network with these parameters and save the labelvec length to be used later. NOTE: intuitively, we would expect the length of the labelvector to equal the length of the input spectrogram in the horizontal (time) dimension; however, as discussed, this software supports processing multiple spectrogram \"channels\" of different aspect ratios (time-frequency resolutions). Typically, we would not downsample in the time dimension, but in some cases we might want to hit specific dimensions across all input channels. Being sensitive to these cases we run a run a test input through the network on instantiation and confirm labelvector size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.network import MultiChannelTweetynet\n",
    "tweetynet = MultiChannelTweetynet(**NETWORK_PARAMS)\n",
    "labelvec_len = tweetynet.labelvec_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labelvec_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Tweetynet Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweetynet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Labelvectors\n",
    "For training and evalution, each spectrogram and spectrogram window needs a labelvector. This method writes labelvectors to disk both for complete spectrograms and for shorter training windows. \n",
    "\n",
    "This step will progress slightly faster than the spectrogram generation step. Though the computational intensity of labelvector generation depends on total downsampling that occurs through convolution and is thus dependent on user provided pooling parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from src.labelvec_writer import LabelVecWriter\n",
    "from parameters.labelvec_writer_params import LABELVEC_WRITER_PARAMS\n",
    "LABELVEC_WRITER_PARAMS[\"labelvec_len\"] = labelvec_len\n",
    "labelvec_writer = LabelVecWriter(**LABELVEC_WRITER_PARAMS)\n",
    "labelvec_writer.write()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training and Evaluation Datasets\n",
    "\n",
    "Note: `sample_train_eval_files` randomly selects a subset of the data we have generated. In this case, we choose all training windows associated with 40 randomly selected audio files and then create an evaluation set from 10 additional files. The returned training and eval sets do not overlap. \n",
    "\n",
    "Try indexing into ```train_dataset``` and ```eval_dataset``` to inspect how training and evalution samples are structured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset import EvalDataset, TrainDataset\n",
    "from parameters.dataset_params import EVAL_DATASET_PARAMS, TRAIN_DATASET_PARAMS\n",
    "from parameters.params import AUDIO_FILE_FMT, UNCUT_SPECTS_DIR_PATH\n",
    "from src.utilities import sample_train_eval_files\n",
    "train_files_list, eval_files_list = sample_train_eval_files(\n",
    "    uncut_spect_path=UNCUT_SPECTS_DIR_PATH,\n",
    "    num_train_files=1,\n",
    "    num_eval_files=1,\n",
    "    audio_file_fmt=AUDIO_FILE_FMT,\n",
    "    spect_file_fmt=SPECT_FILE_FMT,\n",
    ")\n",
    "TRAIN_DATASET_PARAMS['audio_files_list'] = train_files_list_list\n",
    "EVAL_DATASET_PARAMS['spect_files_list'] = eval_files_list\n",
    "train_dataset = TrainDataset(**TRAIN_DATASET_PARAMS)\n",
    "eval_dataset = EvalDataset(**EVAL_DATASET_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp = train_dataset[0]\n",
    "windows, labvec = samp\n",
    "print(windows)\n",
    "print('\\n')\n",
    "print(labvec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training Parameters, Instantiate DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parameters.train_params import DEVICE, NUM_EPOCHS, TRAIN_BATCH_SIZE, EVAL_BATCH_SIZE, NUM_WORKERS, EVAL_STEP, LR\n",
    "from torch.utils.data import DataLoader\n",
    "train_data = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True)\n",
    "eval_data = DataLoader(eval_dataset, batch_size=EVAL_BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=False)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate Model\n",
    "Note, in ```model_params``` below we include the network (tweetynet) that was setup at an earlier step in this notebook.\n",
    "\n",
    "Users are encouraged to write their own model classes or modify the provided class if they have specific desires with respect to saving model ouptuts or internal state throughout training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import TweetynetModel\n",
    "model = TweetynetModel(device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training Loop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "num_train_samps, accs = model.run(\n",
    "    eval_step=EVAL_STEP,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    train_data=train_data,\n",
    "    eval_data=eval_data,\n",
    "    net=tweetynet,\n",
    "    lr=LR,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "fig = plt.figure(figsize=(8,4))\n",
    "plt.title(\"TweetyNet Classification\")\n",
    "plt.xlabel(\"Number of training windows\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.plot(num_train_samps, accs)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "8f7a4aad59252999e7a4d83bf3b7710a8e67957a113c458cbc6a13e8c75aefbc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
