{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('tweetynet': conda)",
   "metadata": {
    "interpreter": {
     "hash": "949c6690c81676a3e4c994a9a8fe96551ac441c27af5a1a9e29acca9b8217277"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Overview\n",
    "This notebooks illustrates usage of the Tweetynet convolutional neural network for birdsong syllable classification and supporting software. The network processes magnitude spectral windows of raw audio recordings by extracting visual features, downsampling the input via 2D convolution and pooling operations, and finally providing downsampled activation maps as input to an LSTM, which feeds a linear readout unit that classifies discrete time steps as belonging either one of the pre-specified syllable types or a period of non-singing. In this notebook illustrate how to generate magnitude spectrograms for a variety of parameterizations, generate labelvectors from song annotation files, create datasets for training and evalution, and build + train a model.\n",
    "\n",
    "This software's unique contribution is in its support of \"wideband\" spectral input to the Tweetynet neural network. More concretely, our implementation of Tweetynet can process a 3D input spectrogram window where depth is created by stacking copies of the same window that are computed with different FFT parameterizations. Tweetynet applies a distinct layer 1 convolution and pooling operation to each channel to appropriately featurize and downsample input to matching dimensions prior to the second convolutional layer. In addition, provided tooling for spectrogram and labelvector generation are easily parameterized with wideband input in mind. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Getting Started\n",
    "Before starting visit and review ```/parameters/param.py```. Said file specifies key parameters of this script and its supporting software. At minimum, you will need to hardcode the following named directory parameters:\n",
    "- ```audio_dir_path```\n",
    "- ```annot_dir_path```\n",
    "- ```spect_dir_path```\n",
    "- ```windowed_spects_dir_path```\n",
    "- ```windowed_labelvecs_dir_path```\n",
    "- ```uncut_spects_dir_path```\n",
    "- ```uncut_labelvecs_dir_path```\n",
    "\n",
    "As you work your way through this notebook consider re-referencing ```/parameters/params.py``` along with other ```_params.py``` files"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Setup Directories\n",
    "Create necessary directories if need be or remove old files from existing directories. Ensure you have already assigned the above listed named directory parameters."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parameters.params import (\n",
    "    windowed_labelvecs_dir_path,\n",
    "    uncut_labelvecs_dir_path,\n",
    "    windowed_spects_dir_path,\n",
    "    uncut_spects_dir_path,\n",
    ")\n",
    "dirs = [\n",
    "    windowed_spects_dir_path,\n",
    "    windowed_labelvecs_dir_path,\n",
    "    uncut_labelvecs_dir_path,\n",
    "    uncut_spects_dir_path,\n",
    "]\n",
    "from src.utilities import setup_directories\n",
    "setup_directories(dirs)"
   ]
  },
  {
   "source": [
    "## Write Spectrograms\n",
    "For each provided audio file we compute and save to disk a full duration spectrogram as well as consecutive overlapping windows. Short windows are useful for training. Full duration spectrograms can be used for reference and/or model evalution on song-by-song basis. As discussed, this version of Tweetynet supports processing multiple input spectrograms as \"channels\". Accordingly, with a single audio file it is easy to generate multiple spectrograms using different parameterizations to render different output dimensions. Our spectrogram generation procedure includes small quirks to produce output images with an identical number of pixels for different FFT setups. For example, with spectrogram A, B, and C, produced with FFT sizes 256, 512, and 1024, the height (frequency dimension) of C will be twice that of B and four times that of A. An identical relationship holds for the time dimension.\n",
    "\n",
    "NOTE: In addition to actual spectrograms and spectrogram windows this method writes other metadata to disk with each record. Details follow.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.spect_writer import SpectWriter\n",
    "from parameters.spect_writer_params import spect_writer_params\n",
    "spect_writer = SpectWriter(**spect_writer_params)\n",
    "spect_writer.write()"
   ]
  },
  {
   "source": [
    "## Determine Actual Spectrogram Window Sizes \n",
    "In order to instantiate our network, we need to provide input spectrogram shapes. Here, use a utility method to load a sample set of spectrogram windows, extract the shapes, and save them to the 'network_params' dictionary."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from src.utilities import get_spect_window_shapes\n",
    "from parameters.params import windowed_spects_dir_path, spect_file_fmt\n",
    "from parameters.net_params import network_params\n",
    "n_ffts = network_params[\"n_ffts\"]\n",
    "network_input_shapes = get_spect_window_shapes(windowed_spects_dir_path, spect_file_fmt, n_ffts)\n",
    "network_params[\"input_shapes\"] = network_input_shapes"
   ]
  },
  {
   "source": [
    "## Create Network. Sample 'forward()' Call To Compute Labelvec Length\n",
    "To create label vectors from audio file anntations we need to know the network's output sizes for predetermined input sizes. Instiate the network with these parameters and save the labelvec length to be used later. NOTE: intuitively, we would expect the length of the labelvector to equal the length of the input spectrogram in the horizontal (time) dimension; however, as discussed, this software supports processing multiple spectrogram \"channels\" of different aspect ratios (time-frequency resolutions). Typically, we would not downsample in the time dimension, but in some cases we might want to hit specific dimensions across all input channels. Being sensitive to these cases we run a run a test input through the network on instantiation and confirm labelvector size."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.network import MultiChannelTweetynet\n",
    "tweetynet = MultiChannelTweetynet(**network_params)\n",
    "labelvec_len = tweetynet.labelvec_len"
   ]
  },
  {
   "source": [
    "## View Tweetynet Architecture"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MultiChannelTweetynet(\n  (cnn1_layers): ModuleDict(\n    (256): DataParallel(\n      (module): Sequential(\n        (0): PadSame()\n        (1): Conv2d(1, 32, kernel_size=(8, 4), stride=(1, 1))\n        (2): ReLU(inplace=True)\n        (3): MaxPool2d(kernel_size=(8, 4), stride=(8, 4), padding=0, dilation=1, ceil_mode=False)\n      )\n    )\n    (512): DataParallel(\n      (module): Sequential(\n        (0): PadSame()\n        (1): Conv2d(1, 32, kernel_size=(16, 2), stride=(1, 1))\n        (2): ReLU(inplace=True)\n        (3): MaxPool2d(kernel_size=(16, 2), stride=(16, 2), padding=0, dilation=1, ceil_mode=False)\n      )\n    )\n    (1024): DataParallel(\n      (module): Sequential(\n        (0): PadSame()\n        (1): Conv2d(1, 32, kernel_size=(32, 1), stride=(1, 1))\n        (2): ReLU(inplace=True)\n        (3): MaxPool2d(kernel_size=(32, 1), stride=(32, 1), padding=0, dilation=1, ceil_mode=False)\n      )\n    )\n  )\n  (cnn2): DataParallel(\n    (module): Sequential(\n      (0): PadSame()\n      (1): Conv2d(96, 64, kernel_size=(3, 3), stride=(1, 1))\n      (2): ReLU(inplace=True)\n      (3): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n    )\n  )\n  (rnn): LSTM(256, 256, bidirectional=True)\n  (fc): Linear(in_features=512, out_features=12, bias=True)\n)\n"
     ]
    }
   ],
   "source": [
    "print(tweetynet)"
   ]
  },
  {
   "source": [
    "## Write Labelvectors\n",
    "For training and evalution, each spectrogram and spectrogram window needs a labelvector. This method writes labelvectors to disk both for complete spectrograms and for shorter training windows. \n",
    "\n",
    "\n",
    "NOTE: For complete spectrograms we write two versions of the labelvector to disk. Details follow."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from src.labelvec_writer import LabelVecWriter\n",
    "from parameters.labelvec_writer_params import labelvec_writer_params\n",
    "labelvec_writer_params[\"labelvec_len\"] = labelvec_len\n",
    "labelvec_writer = LabelVecWriter(**labelvec_writer_params)\n",
    "labelvec_writer.write()"
   ]
  },
  {
   "source": [
    "## Create Training and Evaluation Datasets\n",
    "Try indexing into ```train_dataset``` and ```eval_dataset``` to inspect how training and evalution samples are structured."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from src.dataset import EvalDataset, TrainDataset\n",
    "from parameters.dataset_params import eval_dataset_params, train_dataset_params\n",
    "train_dataset = TrainDataset(**train_dataset_params)\n",
    "eval_dataset = EvalDataset(**eval_dataset_params)\n"
   ]
  },
  {
   "source": [
    "## Load Training Parameters, Instantiate DataLoaders"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parameters.train_params import device, num_epochs, train_batch_size, eval_batch_size, num_workers, eval_step, lr\n",
    "from torch.utils.data import DataLoader\n",
    "train_data = DataLoader(train_dataset, batch_size=train_batch_size, num_workers=num_workers, shuffle=True)\n",
    "eval_data = DataLoader(eval_dataset, batch_size=eval_batch_size, num_workers=num_workers, shuffle=False)   "
   ]
  },
  {
   "source": [
    "## Instantiate Model\n",
    "Note, in ```model_params``` below we include the network (tweetynet) that was setup at an earlier step in this notebook."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import TweetynetModel\n",
    "model_params = {\n",
    "    \"network\": tweetynet,\n",
    "    \"train_data\": train_data,\n",
    "    \"eval_data\": eval_data,\n",
    "    \"device\": device,\n",
    "    \"lr\": lr,\n",
    "}\n",
    "model = TweetynetModel(**model_params)"
   ]
  },
  {
   "source": [
    "## Start Train Loop"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "---------------- EPOCH 1 \n",
      "\n",
      "\n",
      "[0.6184006211180124, 0.5958751393534002, 0.6606280193236715, 0.6065217391304348, 0.625, 0.5795932678821879, 0.5750679347826086, 0.5591032608695652]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.6025237478074852"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "model.train(eval_step=eval_step, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}